{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Clustering Exploratory Analysis\n",
    "\n",
    "This notebook demonstrates the exploratory data analysis and clustering process for stock price data.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path().absolute().parent / \"src\"))\n",
    "\n",
    "from src.data_fetcher import DataFetcher\n",
    "from src.feature_extractor import FeatureExtractor\n",
    "from src.clustering import StockClustering\n",
    "from src.visualizer import ClusterVisualizer\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Fetching Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data fetcher\n",
    "data_fetcher = DataFetcher(cache_dir=\"../data/raw\")\n",
    "\n",
    "# Test with a small sample of symbols\n",
    "test_symbols = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN']\n",
    "\n",
    "print(\"Fetching data for test symbols...\")\n",
    "stock_data_dict = data_fetcher.fetch_multiple_stocks_data(\n",
    "    test_symbols, \n",
    "    period=\"2y\",  # 2 years for faster testing\n",
    "    validate_symbols=True\n",
    ")\n",
    "\n",
    "print(f\"Successfully fetched data for {len(stock_data_dict)} symbols\")\n",
    "print(f\"Symbols: {list(stock_data_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data\n",
    "if stock_data_dict:\n",
    "    combined_data = data_fetcher.combine_all_data(stock_data_dict)\n",
    "    print(f\"Combined data shape: {combined_data.shape}\")\n",
    "    print(f\"Columns: {list(combined_data.columns)}\")\n",
    "    print(f\"Date range: {combined_data['date'].min()} to {combined_data['date'].max()}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    display(combined_data.head())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nBasic statistics:\")\n",
    "    display(combined_data.describe())\n",
    "else:\n",
    "    print(\"No data fetched to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stock_data_dict:\n",
    "    # Initialize feature extractor\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"Extracting features...\")\n",
    "    features_with_data = feature_extractor.extract_features_for_clustering(combined_data)\n",
    "    \n",
    "    print(f\"Features extracted. Shape: {features_with_data.shape}\")\n",
    "    print(f\"Feature columns: {[col for col in features_with_data.columns if col not in ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']]}\")\n",
    "    \n",
    "    # Display sample of features\n",
    "    print(\"\\nSample of extracted features:\")\n",
    "    feature_cols = [col for col in features_with_data.columns if col not in ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    display(features_with_data[['symbol', 'date'] + feature_cols[:10]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fluctuation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stock_data_dict:\n",
    "    # Analyze fluctuation patterns (30-70% ranges)\n",
    "    fluctuation_stats = feature_extractor.count_fluctuation_cycles(\n",
    "        combined_data, \n",
    "        min_pct=30, \n",
    "        max_pct=70\n",
    "    )\n",
    "    \n",
    "    print(\"Fluctuation Analysis (30-70% ranges):\")\n",
    "    for symbol, stats in fluctuation_stats.items():\n",
    "        print(f\"\\n{symbol}:\")\n",
    "        print(f\"  Fluctuation count: {stats['fluctuation_count']}\")\n",
    "        print(f\"  Average period: {stats['avg_fluctuation_period']:.1f} days\")\n",
    "        print(f\"  Frequency: {stats['fluctuation_frequency']:.2f} per year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stock_data_dict:\n",
    "    # Create feature matrix for clustering\n",
    "    feature_matrix = feature_extractor.create_feature_matrix(\n",
    "        features_with_data, \n",
    "        features_per_symbol=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "    print(f\"Feature matrix columns: {list(feature_matrix.columns)}\")\n",
    "    \n",
    "    # Prepare features for clustering\n",
    "    clustering_features, scaler = feature_extractor.prepare_features_for_clustering(\n",
    "        feature_matrix\n",
    "    )\n",
    "    \n",
    "    print(f\"Clustering features shape: {clustering_features.shape}\")\n",
    "    \n",
    "    # Display feature matrix\n",
    "    display(feature_matrix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stock_data_dict and len(feature_matrix) > 2:\n",
    "    # Initialize clustering analyzer\n",
    "    clustering_analyzer = StockClustering(max_clusters=5)  # Smaller for testing\n",
    "    \n",
    "    # Find optimal number of clusters\n",
    "    print(\"Finding optimal number of clusters...\")\n",
    "    cluster_results = clustering_analyzer.find_optimal_clusters(\n",
    "        clustering_features, \n",
    "        cluster_range=range(2, min(6, len(feature_matrix)))  # Test 2-5 clusters\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCluster evaluation results:\")\n",
    "    for n_clusters, metrics in cluster_results.items():\n",
    "        print(f\"{n_clusters} clusters: Silhouette={metrics['silhouette_score']:.3f}, \"\n",
    "              f\"Calinski-Harabasz={metrics['calinski_harabasz_score']:.1f}, \"\n",
    "              f\"Davies-Bouldin={metrics['davies_bouldin_score']:.3f}\")\n",
    "    \n",
    "    # Select best and perform clustering\n",
    "    best_n_clusters = clustering_analyzer.select_best_n_clusters(cluster_results)\n",
    "    print(f\"\\nSelected optimal clusters: {best_n_clusters}\")\n",
    "    \n",
    "    # Perform clustering\n",
    "    cluster_labels = clustering_analyzer.perform_clustering(\n",
    "        clustering_features,\n",
    "        n_clusters=best_n_clusters,\n",
    "        auto_optimize=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nClustering completed. Labels: {cluster_labels}\")\n",
    "    \n",
    "    # Analyze clusters\n",
    "    cluster_analysis = clustering_analyzer.analyze_clusters(feature_matrix, cluster_labels)\n",
    "    \n",
    "    print(\"\\nCluster analysis:\")\n",
    "    for cluster_id, stats in cluster_analysis.items():\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        print(f\"  Size: {stats['size']} stocks\")\n",
    "        print(f\"  Symbols: {', '.join(stats.get('symbols', []))}\")\n",
    "        print(f\"  Avg volatility: {stats.get('volatility_252d_mean', 0):.3f}\")\n",
    "        print(f\"  Avg return: {stats.get('return_mean', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stock_data_dict and len(feature_matrix) > 2:\n",
    "    # Initialize visualizer\n",
    "    visualizer = ClusterVisualizer(output_dir=\"../reports\")\n",
    "    \n",
    "    # Get cluster assignments\n",
    "    cluster_assignments = clustering_analyzer.get_cluster_assignments(feature_matrix)\n",
    "    \n",
    "    # Reduce dimensions for visualization\n",
    "    features_2d, _ = clustering_analyzer.reduce_dimensions(\n",
    "        clustering_features, method='pca', n_components=2\n",
    "    )\n",
    "    \n",
    "    # Generate descriptive labels\n",
    "    descriptive_labels = clustering_analyzer.generate_cluster_labels(cluster_analysis)\n",
    "    print(\"\\nDescriptive cluster labels:\")\n",
    "    for cluster_id, label in descriptive_labels.items():\n",
    "        print(f\"  Cluster {cluster_id}: {label}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    plots_created = visualizer.create_comprehensive_report(\n",
    "        features_2d=features_2d,\n",
    "        cluster_labels=cluster_labels,\n",
    "        cluster_assignments=cluster_assignments,\n",
    "        cluster_analysis=cluster_analysis,\n",
    "        clustering_metrics=clustering_analyzer.evaluate_clustering_quality(clustering_features, cluster_labels),\n",
    "        stock_data=combined_data,\n",
    "        method='PCA'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nVisualizations created:\")\n",
    "    for plot_name, plot_path in plots_created.items():\n",
    "        print(f\"  {plot_name}: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stock_data_dict:\n",
    "    print(\"=== STOCK CLUSTERING ANALYSIS SUMMARY ===\")\n",
    "    print(f\"\\nData Overview:\")\n",
    "    print(f\"  - Symbols analyzed: {len(stock_data_dict)}\")\n",
    "    print(f\"  - Data points per symbol: {len(combined_data) // len(stock_data_dict)}\")\n",
    "    print(f\"  - Date range: {combined_data['date'].min().strftime('%Y-%m-%d')} to {combined_data['date'].max().strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    print(f\"\\nFeature Engineering:\")\n",
    "    print(f\"  - Total features extracted: {len([col for col in features_with_data.columns if col not in ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']])}\")\n",
    "    print(f\"  - Feature matrix shape: {feature_matrix.shape}\")\n",
    "    \n",
    "    if len(feature_matrix) > 2:\n",
    "        print(f\"\\nClustering Results:\")\n",
    "        print(f\"  - Optimal clusters found: {best_n_clusters}\")\n",
    "        print(f\"  - Clustering quality metrics calculated\")\n",
    "        \n",
    "        for cluster_id, stats in cluster_analysis.items():\n",
    "            label = descriptive_labels.get(cluster_id, f\"Cluster {cluster_id}\")\n",
    "            print(f\"  - {label}: {stats['size']} stocks\")\n",
    "    \n",
    "    print(f\"\\nFluctuation Analysis (30-70% ranges):\")\n",
    "    for symbol, stats in fluctuation_stats.items():\n",
    "        if stats['fluctuation_count'] > 0:\n",
    "            print(f\"  - {symbol}: {stats['fluctuation_count']} cycles, avg period {stats['avg_fluctuation_period']:.0f} days\")\n",
    "        else:\n",
    "            print(f\"  - {symbol}: No significant fluctuations detected\")\n",
    "    \n",
    "    print(f\"\\nOutputs Generated:\")\n",
    "    if 'plots_created' in locals():\n",
    "        for plot_name, plot_path in plots_created.items():\n",
    "            print(f\"  - {plot_name}: {plot_path}\")\n",
    "else:\n",
    "    print(\"No data available for analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
